{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from Enviroments.Continuous_Env import DroneEnv\n",
    "from Algs.agent import  Agent\n",
    "import time\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "enc_path = \"enc.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\research\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 4\n",
      "Transformer.pth\n"
     ]
    }
   ],
   "source": [
    "runtime = 5\n",
    "dt = 0.001\n",
    "env = DroneEnv(runtime = runtime, dt = dt)\n",
    "max_t = int(runtime/dt)\n",
    "n_episodes = 50\n",
    "obs = env.reset()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "hidden_size = 128\n",
    "batch_size = 256\n",
    "# model_type = 'LSTM'\n",
    "model_type = 'Transformer'\n",
    "enc_path = model_type + \".pth\"\n",
    "print(state_size, action_size)\n",
    "print(enc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 40\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# run the training loop\u001B[39;00m\n\u001B[0;32m     39\u001B[0m agent \u001B[38;5;241m=\u001B[39m Agent(state_size\u001B[38;5;241m=\u001B[39mstate_size,hidden_size\u001B[38;5;241m=\u001B[39mhidden_size, random_seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m,env \u001B[38;5;241m=\u001B[39m env,batch_size \u001B[38;5;241m=\u001B[39m batch_size,model_type\u001B[38;5;241m=\u001B[39mmodel_type)\n\u001B[1;32m---> 40\u001B[0m losses, avgs \u001B[38;5;241m=\u001B[39m \u001B[43mforcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_episodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_t\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolved_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_every\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43menc_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menc_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure()\n\u001B[0;32m     42\u001B[0m ax \u001B[38;5;241m=\u001B[39m fig\u001B[38;5;241m.\u001B[39madd_subplot(\u001B[38;5;241m111\u001B[39m)\n",
      "Cell \u001B[1;32mIn [3], line 12\u001B[0m, in \u001B[0;36mforcast\u001B[1;34m(n_episodes, max_t, best_score, consec_episodes, print_every, solved_score, train_mode, enc_path, critic_path)\u001B[0m\n\u001B[0;32m      8\u001B[0m avgs \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# list of averages\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_episode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_episodes \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 12\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_t\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i_episode \u001B[38;5;241m%\u001B[39m print_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     15\u001B[0m         start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\OneDrive - Technion\\Reserch\\EncoderSim\\Algs\\agent.py:83\u001B[0m, in \u001B[0;36mAgent.step\u001B[1;34m(self, max_t)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_t):\n\u001B[0;32m     82\u001B[0m     action \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m+\u001B[39m \u001B[38;5;241m3\u001B[39m\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;241m4\u001B[39m)\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39msin(\u001B[38;5;241m0.5\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m6.28\u001B[39m\u001B[38;5;241m*\u001B[39mt)\n\u001B[1;32m---> 83\u001B[0m     obs, r, d, i \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# send actions to environment\u001B[39;00m\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mloc[t] \u001B[38;5;241m=\u001B[39m obs\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearn()\n",
      "File \u001B[1;32m~\\OneDrive - Technion\\Reserch\\EncoderSim\\Enviroments\\Continuous_Env.py:98\u001B[0m, in \u001B[0;36mDroneEnv.step\u001B[1;34m(self, action, ref_cmd)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mControl_cmd \u001B[38;5;241m=\u001B[39m Volt_Input_cmd\n\u001B[0;32m     97\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_obs(action\u001B[38;5;241m/\u001B[39mK_action)\n\u001B[1;32m---> 98\u001B[0m reward \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcopy(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_reward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39msingle)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     99\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_done(falling)\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# Optionally we can pass additional info, we are not using that for now\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - Technion\\Reserch\\EncoderSim\\Enviroments\\Continuous_Env.py:177\u001B[0m, in \u001B[0;36mDroneEnv.get_reward\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    175\u001B[0m pos_err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPos_err\n\u001B[0;32m    176\u001B[0m phase_err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPhase_err\n\u001B[1;32m--> 177\u001B[0m phase_err \u001B[38;5;241m=\u001B[39m \u001B[43mFold2HalfPi\u001B[49m\u001B[43m(\u001B[49m\u001B[43mphase_err\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    178\u001B[0m stable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mall\u001B[39m(np\u001B[38;5;241m.\u001B[39mabs(pos_err) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.25\u001B[39m)\n\u001B[0;32m    179\u001B[0m balance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mall\u001B[39m(np\u001B[38;5;241m.\u001B[39mabs(phase_err) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m10\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m0.0174\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(np\u001B[38;5;241m.\u001B[39mabs(pos_err) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive - Technion\\Reserch\\EncoderSim\\Physics_Models\\ControlAlgTestMod.py:128\u001B[0m, in \u001B[0;36mFold2HalfPi\u001B[1;34m(u)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mFold2HalfPi\u001B[39m(u):\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;66;03m#     if u > np.pi/2:\u001B[39;00m\n\u001B[1;32m--> 128\u001B[0m     u \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    129\u001B[0m     relv_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(u \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    130\u001B[0m     u[relv_idx] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m-\u001B[39m u[relv_idx] \u001B[38;5;241m%\u001B[39m (np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forcast(n_episodes=5000, max_t=10000, best_score = 1000, consec_episodes = 5, print_every=100,solved_score = 1e5, train_mode=True,\n",
    "         enc_path='enc.pth', critic_path='critic.pth'):\n",
    "\n",
    "    mean_loss = []  # list of mean scores from each episode\n",
    "    loss_window = deque(maxlen=consec_episodes)  # mean scores from most recent episodes\n",
    "    avgs = []  # list of averages\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "\n",
    "        agent.step(max_t)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            start_time = time.time()\n",
    "            loss = agent.test(max_t)\n",
    "            duration = time.time() - start_time\n",
    "            mean_loss.append(np.mean(loss))  # save mean score for the episode\n",
    "            loss_window.append(mean_loss[-1])  # save mean score to window\n",
    "            avgs.append(np.mean(loss_window))  # save moving average\n",
    "            print('\\rEpisode {} ({} sec)  -- \\tMean Loss ={:.1f} -- \\t Avg: {:.1f}'.format( \\\n",
    "                i_episode, round(duration), mean_loss[-1]*100, avgs[-1]*100,))\n",
    "\n",
    "            if avgs[-1] <= best_score and i_episode >= consec_episodes:\n",
    "                print('\\nEnvironment SOLVED in {} episodes!\\tMoving Average ={:.1f} over last {} episodes'.format( \\\n",
    "                    i_episode - consec_episodes, mean_loss[-1], avgs[-1], consec_episodes))\n",
    "                if train_mode:\n",
    "                    torch.save({'model_state_dict': agent.model.state_dict(),\n",
    "                                'optimizer_state_dict': agent.optimizer.state_dict()}, enc_path)\n",
    "\n",
    "                best_score = avgs[-1]\n",
    "                # if best_score < solved_score:\n",
    "                #     break\n",
    "\n",
    "    return mean_loss, avgs\n",
    "\n",
    "# run the training loop\n",
    "\n",
    "agent = Agent(state_size=state_size,hidden_size=hidden_size, random_seed=42,env = env,batch_size = batch_size,model_type=model_type)\n",
    "losses, avgs = forcast(n_episodes=n_episodes, max_t=max_t, solved_score=0, print_every=10,enc_path=enc_path)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(losses)), losses, label='loss')\n",
    "plt.plot(np.arange(len(losses)), avgs, c='r', label='moving avg')\n",
    "plt.legend()\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# transformer_model = nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048,)\n",
    "#\n",
    "# out = transformer_model(src, tgt)\n",
    "# print(out)\n",
    "\n",
    "# Atten = nn.MultiheadAttention(embed_dim = 32, num_heads =4)\n",
    "# x = torch.rand((32,))\n",
    "# Atten.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}